//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_75
.address_size 64

	// .globl	enhance_kernel

.visible .entry enhance_kernel(
	.param .u64 enhance_kernel_param_0,
	.param .u64 enhance_kernel_param_1,
	.param .u32 enhance_kernel_param_2,
	.param .u32 enhance_kernel_param_3,
	.param .u32 enhance_kernel_param_4,
	.param .u32 enhance_kernel_param_5,
	.param .f32 enhance_kernel_param_6,
	.param .f32 enhance_kernel_param_7
)
{
	.reg .pred 	%p<37>;
	.reg .b16 	%rs<34>;
	.reg .f32 	%f<255>;
	.reg .b32 	%r<64>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd13, [enhance_kernel_param_0];
	ld.param.u64 	%rd12, [enhance_kernel_param_1];
	ld.param.u32 	%r25, [enhance_kernel_param_2];
	ld.param.u32 	%r26, [enhance_kernel_param_3];
	ld.param.f32 	%f107, [enhance_kernel_param_6];
	ld.param.f32 	%f108, [enhance_kernel_param_7];
	cvta.to.global.u64 	%rd1, %rd13;
	mov.u32 	%r27, %ctaid.x;
	mov.u32 	%r28, %ntid.x;
	mov.u32 	%r29, %tid.x;
	mad.lo.s32 	%r1, %r28, %r27, %r29;
	mov.u32 	%r30, %ctaid.y;
	mov.u32 	%r31, %ntid.y;
	mov.u32 	%r32, %tid.y;
	mad.lo.s32 	%r2, %r31, %r30, %r32;
	setp.ge.s32 	%p1, %r1, %r25;
	setp.ge.s32 	%p2, %r2, %r26;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_28;

	add.s32 	%r3, %r2, -1;
	add.s32 	%r4, %r1, -1;
	or.b32  	%r34, %r4, %r3;
	setp.lt.s32 	%p4, %r34, 0;
	setp.gt.s32 	%p5, %r1, %r25;
	or.pred  	%p6, %p5, %p4;
	setp.gt.s32 	%p7, %r2, %r26;
	or.pred  	%p8, %p7, %p6;
	mad.lo.s32 	%r35, %r3, %r25, %r1;
	mul.wide.s32 	%rd14, %r35, 2;
	add.s64 	%rd2, %rd1, %rd14;
	mul.lo.s32 	%r36, %r26, %r25;
	mul.wide.s32 	%rd15, %r36, 2;
	mov.u32 	%r56, 1;
	add.s64 	%rd3, %rd2, %rd15;
	add.s64 	%rd4, %rd3, %rd15;
	mov.f32 	%f207, 0f00000000;
	mov.f32 	%f208, %f207;
	mov.f32 	%f209, %f207;
	mov.f32 	%f210, %f207;
	mov.f32 	%f212, %f207;
	@%p8 bra 	$L__BB0_3;

	ld.global.u16 	%rs1, [%rd2+-2];
	// begin inline asm
	{  cvt.f32.f16 %f114, %rs1;}

	// end inline asm
	ld.global.u16 	%rs2, [%rd3+-2];
	// begin inline asm
	{  cvt.f32.f16 %f115, %rs2;}

	// end inline asm
	ld.global.u16 	%rs3, [%rd4+-2];
	// begin inline asm
	{  cvt.f32.f16 %f116, %rs3;}

	// end inline asm
	mul.f32 	%f117, %f115, 0f3F1645A2;
	fma.rn.f32 	%f118, %f114, 0f3E991687, %f117;
	fma.rn.f32 	%f119, %f116, 0f3DE978D5, %f118;
	add.f32 	%f207, %f119, 0f00000000;
	mov.f32 	%f120, 0f00000000;
	add.f32 	%f208, %f114, 0f00000000;
	add.f32 	%f209, %f115, 0f00000000;
	add.f32 	%f210, %f116, 0f00000000;
	sub.f32 	%f212, %f120, %f119;
	mov.u32 	%r56, 0;

$L__BB0_3:
	or.b32  	%r38, %r1, %r3;
	setp.lt.s32 	%p9, %r38, 0;
	or.pred  	%p11, %p7, %p9;
	@%p11 bra 	$L__BB0_5;
	bra.uni 	$L__BB0_4;

$L__BB0_5:
	add.s32 	%r56, %r56, 1;
	mov.f32 	%f217, %f212;
	bra.uni 	$L__BB0_6;

$L__BB0_4:
	ld.global.u16 	%rs4, [%rd2];
	// begin inline asm
	{  cvt.f32.f16 %f121, %rs4;}

	// end inline asm
	ld.global.u16 	%rs5, [%rd3];
	// begin inline asm
	{  cvt.f32.f16 %f122, %rs5;}

	// end inline asm
	ld.global.u16 	%rs6, [%rd4];
	// begin inline asm
	{  cvt.f32.f16 %f123, %rs6;}

	// end inline asm
	mul.f32 	%f124, %f122, 0f3F1645A2;
	fma.rn.f32 	%f125, %f121, 0f3E991687, %f124;
	fma.rn.f32 	%f126, %f123, 0f3DE978D5, %f125;
	add.f32 	%f207, %f207, %f126;
	add.f32 	%f208, %f208, %f121;
	add.f32 	%f209, %f209, %f122;
	add.f32 	%f210, %f210, %f123;
	fma.rn.f32 	%f217, %f126, 0f00000000, %f212;
	add.f32 	%f127, %f126, %f126;
	sub.f32 	%f212, %f212, %f127;

$L__BB0_6:
	add.s32 	%r8, %r1, 1;
	setp.ge.s32 	%p12, %r8, %r25;
	or.b32  	%r39, %r8, %r3;
	setp.lt.s32 	%p13, %r39, 0;
	or.pred  	%p14, %p12, %p13;
	or.pred  	%p16, %p7, %p14;
	@%p16 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_7;

$L__BB0_8:
	add.s32 	%r56, %r56, 1;
	bra.uni 	$L__BB0_9;

$L__BB0_7:
	ld.global.u16 	%rs7, [%rd2+2];
	// begin inline asm
	{  cvt.f32.f16 %f128, %rs7;}

	// end inline asm
	ld.global.u16 	%rs8, [%rd3+2];
	// begin inline asm
	{  cvt.f32.f16 %f129, %rs8;}

	// end inline asm
	ld.global.u16 	%rs9, [%rd4+2];
	// begin inline asm
	{  cvt.f32.f16 %f130, %rs9;}

	// end inline asm
	mul.f32 	%f131, %f129, 0f3F1645A2;
	fma.rn.f32 	%f132, %f128, 0f3E991687, %f131;
	fma.rn.f32 	%f133, %f130, 0f3DE978D5, %f132;
	add.f32 	%f207, %f207, %f133;
	add.f32 	%f208, %f208, %f128;
	add.f32 	%f209, %f209, %f129;
	add.f32 	%f210, %f210, %f130;
	add.f32 	%f217, %f217, %f133;
	sub.f32 	%f212, %f212, %f133;

$L__BB0_9:
	or.b32  	%r40, %r4, %r2;
	setp.lt.s32 	%p17, %r40, 0;
	or.pred  	%p19, %p5, %p17;
	mul.lo.s32 	%r11, %r2, %r25;
	add.s32 	%r41, %r11, %r1;
	cvt.s64.s32 	%rd5, %r41;
	mul.wide.s32 	%rd16, %r41, 2;
	add.s64 	%rd6, %rd1, %rd16;
	add.s64 	%rd7, %rd6, %rd15;
	add.s64 	%rd8, %rd7, %rd15;
	@%p19 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_10;

$L__BB0_11:
	add.s32 	%r56, %r56, 1;
	bra.uni 	$L__BB0_12;

$L__BB0_10:
	ld.global.u16 	%rs10, [%rd6+-2];
	// begin inline asm
	{  cvt.f32.f16 %f134, %rs10;}

	// end inline asm
	ld.global.u16 	%rs11, [%rd7+-2];
	// begin inline asm
	{  cvt.f32.f16 %f135, %rs11;}

	// end inline asm
	ld.global.u16 	%rs12, [%rd8+-2];
	// begin inline asm
	{  cvt.f32.f16 %f136, %rs12;}

	// end inline asm
	mul.f32 	%f137, %f135, 0f3F1645A2;
	fma.rn.f32 	%f138, %f134, 0f3E991687, %f137;
	fma.rn.f32 	%f139, %f136, 0f3DE978D5, %f138;
	add.f32 	%f207, %f207, %f139;
	add.f32 	%f208, %f208, %f134;
	add.f32 	%f209, %f209, %f135;
	add.f32 	%f210, %f210, %f136;
	add.f32 	%f140, %f139, %f139;
	sub.f32 	%f217, %f217, %f140;
	fma.rn.f32 	%f212, %f139, 0f00000000, %f212;

$L__BB0_12:
	or.b32  	%r43, %r1, %r2;
	setp.lt.s32 	%p20, %r43, 0;
	@%p20 bra 	$L__BB0_14;

	ld.global.u16 	%rs13, [%rd6];
	// begin inline asm
	{  cvt.f32.f16 %f141, %rs13;}

	// end inline asm
	ld.global.u16 	%rs14, [%rd7];
	// begin inline asm
	{  cvt.f32.f16 %f142, %rs14;}

	// end inline asm
	ld.global.u16 	%rs15, [%rd8];
	// begin inline asm
	{  cvt.f32.f16 %f143, %rs15;}

	// end inline asm
	mul.f32 	%f144, %f142, 0f3F1645A2;
	fma.rn.f32 	%f145, %f141, 0f3E991687, %f144;
	fma.rn.f32 	%f146, %f143, 0f3DE978D5, %f145;
	add.f32 	%f207, %f207, %f146;
	add.f32 	%f208, %f208, %f141;
	add.f32 	%f209, %f209, %f142;
	add.f32 	%f210, %f210, %f143;
	fma.rn.f32 	%f217, %f146, 0f00000000, %f217;
	fma.rn.f32 	%f212, %f146, 0f00000000, %f212;
	bra.uni 	$L__BB0_15;

$L__BB0_14:
	add.s32 	%r56, %r56, 1;

$L__BB0_15:
	or.b32  	%r44, %r8, %r2;
	setp.lt.s32 	%p21, %r44, 0;
	or.pred  	%p23, %p12, %p21;
	@%p23 bra 	$L__BB0_17;
	bra.uni 	$L__BB0_16;

$L__BB0_17:
	add.s32 	%r56, %r56, 1;
	bra.uni 	$L__BB0_18;

$L__BB0_16:
	ld.global.u16 	%rs16, [%rd6+2];
	// begin inline asm
	{  cvt.f32.f16 %f147, %rs16;}

	// end inline asm
	ld.global.u16 	%rs17, [%rd7+2];
	// begin inline asm
	{  cvt.f32.f16 %f148, %rs17;}

	// end inline asm
	ld.global.u16 	%rs18, [%rd8+2];
	// begin inline asm
	{  cvt.f32.f16 %f149, %rs18;}

	// end inline asm
	mul.f32 	%f150, %f148, 0f3F1645A2;
	fma.rn.f32 	%f151, %f147, 0f3E991687, %f150;
	fma.rn.f32 	%f152, %f149, 0f3DE978D5, %f151;
	add.f32 	%f207, %f207, %f152;
	add.f32 	%f208, %f208, %f147;
	add.f32 	%f209, %f209, %f148;
	add.f32 	%f210, %f210, %f149;
	fma.rn.f32 	%f217, %f152, 0f40000000, %f217;
	fma.rn.f32 	%f212, %f152, 0f00000000, %f212;

$L__BB0_18:
	add.s32 	%r18, %r2, 1;
	setp.ge.s32 	%p24, %r18, %r26;
	or.b32  	%r45, %r4, %r18;
	setp.lt.s32 	%p25, %r45, 0;
	or.pred  	%p27, %p5, %p25;
	or.pred  	%p28, %p24, %p27;
	add.s32 	%r46, %r11, %r25;
	add.s32 	%r47, %r1, %r46;
	mul.wide.s32 	%rd18, %r47, 2;
	add.s64 	%rd9, %rd1, %rd18;
	add.s64 	%rd10, %rd9, %rd15;
	add.s64 	%rd11, %rd10, %rd15;
	@%p28 bra 	$L__BB0_20;
	bra.uni 	$L__BB0_19;

$L__BB0_20:
	add.s32 	%r56, %r56, 1;
	bra.uni 	$L__BB0_21;

$L__BB0_19:
	ld.global.u16 	%rs19, [%rd9+-2];
	// begin inline asm
	{  cvt.f32.f16 %f153, %rs19;}

	// end inline asm
	ld.global.u16 	%rs20, [%rd10+-2];
	// begin inline asm
	{  cvt.f32.f16 %f154, %rs20;}

	// end inline asm
	ld.global.u16 	%rs21, [%rd11+-2];
	// begin inline asm
	{  cvt.f32.f16 %f155, %rs21;}

	// end inline asm
	mul.f32 	%f156, %f154, 0f3F1645A2;
	fma.rn.f32 	%f157, %f153, 0f3E991687, %f156;
	fma.rn.f32 	%f158, %f155, 0f3DE978D5, %f157;
	add.f32 	%f207, %f207, %f158;
	add.f32 	%f208, %f208, %f153;
	add.f32 	%f209, %f209, %f154;
	add.f32 	%f210, %f210, %f155;
	sub.f32 	%f217, %f217, %f158;
	add.f32 	%f212, %f212, %f158;

$L__BB0_21:
	or.b32  	%r49, %r1, %r18;
	setp.lt.s32 	%p29, %r49, 0;
	or.pred  	%p31, %p24, %p29;
	@%p31 bra 	$L__BB0_23;
	bra.uni 	$L__BB0_22;

$L__BB0_23:
	add.s32 	%r56, %r56, 1;
	bra.uni 	$L__BB0_24;

$L__BB0_22:
	ld.global.u16 	%rs22, [%rd9];
	// begin inline asm
	{  cvt.f32.f16 %f159, %rs22;}

	// end inline asm
	ld.global.u16 	%rs23, [%rd10];
	// begin inline asm
	{  cvt.f32.f16 %f160, %rs23;}

	// end inline asm
	ld.global.u16 	%rs24, [%rd11];
	// begin inline asm
	{  cvt.f32.f16 %f161, %rs24;}

	// end inline asm
	mul.f32 	%f162, %f160, 0f3F1645A2;
	fma.rn.f32 	%f163, %f159, 0f3E991687, %f162;
	fma.rn.f32 	%f164, %f161, 0f3DE978D5, %f163;
	add.f32 	%f207, %f207, %f164;
	add.f32 	%f208, %f208, %f159;
	add.f32 	%f209, %f209, %f160;
	add.f32 	%f210, %f210, %f161;
	fma.rn.f32 	%f217, %f164, 0f00000000, %f217;
	fma.rn.f32 	%f212, %f164, 0f40000000, %f212;

$L__BB0_24:
	or.b32  	%r50, %r8, %r18;
	setp.lt.s32 	%p32, %r50, 0;
	or.pred  	%p34, %p12, %p32;
	or.pred  	%p36, %p24, %p34;
	@%p36 bra 	$L__BB0_26;
	bra.uni 	$L__BB0_25;

$L__BB0_26:
	add.s32 	%r56, %r56, 1;
	bra.uni 	$L__BB0_27;

$L__BB0_25:
	ld.global.u16 	%rs25, [%rd9+2];
	// begin inline asm
	{  cvt.f32.f16 %f165, %rs25;}

	// end inline asm
	ld.global.u16 	%rs26, [%rd10+2];
	// begin inline asm
	{  cvt.f32.f16 %f166, %rs26;}

	// end inline asm
	ld.global.u16 	%rs27, [%rd11+2];
	// begin inline asm
	{  cvt.f32.f16 %f167, %rs27;}

	// end inline asm
	mul.f32 	%f168, %f166, 0f3F1645A2;
	fma.rn.f32 	%f169, %f165, 0f3E991687, %f168;
	fma.rn.f32 	%f170, %f167, 0f3DE978D5, %f169;
	add.f32 	%f207, %f207, %f170;
	add.f32 	%f208, %f208, %f165;
	add.f32 	%f209, %f209, %f166;
	add.f32 	%f210, %f210, %f167;
	add.f32 	%f217, %f217, %f170;
	add.f32 	%f212, %f212, %f170;

$L__BB0_27:
	cvta.to.global.u64 	%rd20, %rd12;
	ld.global.u16 	%rs28, [%rd6];
	// begin inline asm
	{  cvt.f32.f16 %f171, %rs28;}

	// end inline asm
	add.s32 	%r51, %r2, %r26;
	mad.lo.s32 	%r52, %r51, %r25, %r1;
	mul.wide.s32 	%rd21, %r52, 2;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u16 	%rs29, [%rd22];
	// begin inline asm
	{  cvt.f32.f16 %f172, %rs29;}

	// end inline asm
	mul.lo.s32 	%r53, %r25, %r26;
	shl.b32 	%r54, %r53, 1;
	mul.wide.s32 	%rd23, %r54, 2;
	add.s64 	%rd24, %rd6, %rd23;
	ld.global.u16 	%rs30, [%rd24];
	// begin inline asm
	{  cvt.f32.f16 %f173, %rs30;}

	// end inline asm
	cvt.rn.f32.s32 	%f177, %r56;
	mov.f32 	%f178, 0f41100000;
	sub.f32 	%f179, %f178, %f177;
	div.rn.f32 	%f180, %f207, %f179;
	mul.f32 	%f181, %f172, 0f3F1645A2;
	fma.rn.f32 	%f182, %f171, 0f3E991687, %f181;
	fma.rn.f32 	%f183, %f173, 0f3DE978D5, %f182;
	sub.f32 	%f184, %f183, %f180;
	abs.f32 	%f185, %f184;
	mul.f32 	%f186, %f185, %f107;
	div.rn.f32 	%f187, %f208, %f179;
	sub.f32 	%f188, %f171, %f187;
	fma.rn.f32 	%f189, %f188, %f186, %f171;
	div.rn.f32 	%f190, %f209, %f179;
	sub.f32 	%f191, %f172, %f190;
	fma.rn.f32 	%f192, %f191, %f186, %f172;
	div.rn.f32 	%f193, %f210, %f179;
	sub.f32 	%f194, %f173, %f193;
	fma.rn.f32 	%f195, %f194, %f186, %f173;
	mul.f32 	%f196, %f212, %f212;
	fma.rn.f32 	%f197, %f217, %f217, %f196;
	sqrt.rn.f32 	%f198, %f197;
	fma.rn.f32 	%f199, %f198, %f108, %f189;
	fma.rn.f32 	%f200, %f198, %f108, %f192;
	fma.rn.f32 	%f201, %f198, %f108, %f195;
	cvt.sat.f32.f32 	%f174, %f199;
	cvt.sat.f32.f32 	%f175, %f200;
	cvt.sat.f32.f32 	%f176, %f201;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs31, %f174;}

	// end inline asm
	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd20, %rd25;
	st.global.u16 	[%rd26], %rs31;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs32, %f175;}

	// end inline asm
	add.s64 	%rd27, %rd20, %rd21;
	st.global.u16 	[%rd27], %rs32;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f176;}

	// end inline asm
	add.s64 	%rd28, %rd26, %rd23;
	st.global.u16 	[%rd28], %rs33;

$L__BB0_28:
	ret;

}

